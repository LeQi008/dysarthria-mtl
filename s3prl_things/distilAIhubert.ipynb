{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "368789dc",
   "metadata": {},
   "source": [
    "## Hidden States "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fef4fb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YIDAN\\Desktop\\projects\\dysarthria-mtl-steal\\venvs3prl\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 31, 768])\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# from distilAlhubert.src.upstream.alhubert.expert import UpstreamExpert\n",
    "# when using in s3prl, you can use \n",
    "from s3prl.s3prl.upstream.alhubert.expert import UpstreamExpert\n",
    "model_ckpt_path = \"small.ckpt\"\n",
    "model = UpstreamExpert(model_ckpt_path)\n",
    "data = [torch.randn(10000) for _ in range(2)] # 16KHz\n",
    "states = model(data)\n",
    "print(states[\"last_hidden_state\"].shape) # torch.Tensor: hidden state of the last layer\n",
    "print(len(states[\"hidden_states\"])) # list[torch.Tensor] hidden states of each layer\n",
    "\n",
    "# please note that if layer_norm_first=False (default), \"hidden_states\" will be the outputs of transformer layer 0,1,...11\n",
    "# layer_norm_first=True (for HuBERT Large teachers), \"hidden_states\" will be the outputs of the CNN feature estractor and transformer layer 0,1,...10.\n",
    "# in that case, the output of transformer layer 11 is in states[\"last_hidden_state\"].\n",
    "# This is because that the feature after layer norm is better for distillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e775ab83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu126\n",
      "12.6\n",
      "90701\n",
      "True\n",
      "CUDA is available!\n",
      "GPU Name: NVIDIA GeForce RTX 4060\n",
      "CUDA Version (PyTorch compiled): 12.6\n",
      "CUDA Version (runtime): (8, 9)\n",
      "Device : cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA Version (PyTorch compiled):\", torch.version.cuda)\n",
    "    print(\"CUDA Version (runtime):\", torch.cuda.get_device_capability(0))\n",
    "else:\n",
    "    print(\"CUDA not available.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device : {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da23f9b3",
   "metadata": {},
   "source": [
    "### Freeze audio to vector model params?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e5a9abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3687d5e8",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73a6b88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio  # For loading audio\n",
    "import os\n",
    "import librosa\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, csv_file, split=\"train\"):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.df = self.df[self.df['split'] == split].reset_index(drop=True)\n",
    "        self.sr = 16000\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        path = row['path']\n",
    "        waveform, _ = librosa.load(path, sr=self.sr)\n",
    "        waveform = torch.tensor(waveform, dtype=torch.float)\n",
    "        label = torch.tensor(int(row['category']), dtype=torch.float)\n",
    "        return row['name'], row['path'], waveform, label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    names, paths, waveforms, labels = zip(*batch)\n",
    "    # pad to longest in batch\n",
    "    max_len = max([x.shape[0] for x in waveforms])\n",
    "    padded_waveforms = [torch.nn.functional.pad(x, (0, max_len - x.shape[0])) for x in waveforms]\n",
    "    return names, paths, torch.stack(padded_waveforms), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dddc9bb",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49084bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self, upstream_model, hidden_dim=768):\n",
    "        super().__init__()\n",
    "        self.upstream = upstream_model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)  # binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, wavs):\n",
    "        with torch.no_grad():\n",
    "            states = self.upstream(wavs)\n",
    "            features = states[\"last_hidden_state\"]  # (B, T, 768)\n",
    "\n",
    "        x = features.mean(dim=1)  # (B, 768), mean pooling across time\n",
    "        out = self.classifier(x)  # (B, 1)\n",
    "        return out.squeeze(dim=1)  # (B,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c40fda",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95d223a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for names, paths, wavs, labels in dataloader:\n",
    "        wavs, labels = wavs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(wavs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Accuracy\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).int()\n",
    "        correct += (preds == labels.int()).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb0bea4",
   "metadata": {},
   "source": [
    "## Evaluation + Save Predictions to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed71fa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device, criterion=None, output_csv=None):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_rows = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for names, paths, wavs, labels in dataloader:\n",
    "            wavs, labels = wavs.to(device), labels.to(device)\n",
    "            outputs = model(wavs)\n",
    "\n",
    "            if criterion:\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).int()\n",
    "            correct += (preds == labels.int()).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            if output_csv:\n",
    "                preds_cpu = preds.cpu().tolist()\n",
    "                for name, path, cat, pred in zip(names, paths, labels.cpu(), preds_cpu):\n",
    "                    all_rows.append({\n",
    "                        \"name\": name,\n",
    "                        \"path\": path,\n",
    "                        \"category\": int(cat),\n",
    "                        \"predicted_score\": int(pred)\n",
    "                    })\n",
    "\n",
    "    if output_csv:\n",
    "        pd.DataFrame(all_rows).to_csv(output_csv, index=False)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader) if criterion else None\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ba4d19",
   "metadata": {},
   "source": [
    "## Full training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5612b7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YIDAN\\Desktop\\projects\\dysarthria-mtl-steal\\venvs3prl\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss: 0.6007 | Val Loss: 0.4654 | Train Acc: 0.6889 | Val Acc: 0.7500\n",
      "✅ New best model saved at epoch 0\n",
      "Epoch 1 | Train Loss: 0.4981 | Val Loss: 0.3214 | Train Acc: 0.7156 | Val Acc: 0.8750\n",
      "✅ New best model saved at epoch 1\n",
      "Epoch 2 | Train Loss: 0.4231 | Val Loss: 0.2472 | Train Acc: 0.8168 | Val Acc: 1.0000\n",
      "✅ New best model saved at epoch 2\n",
      "Epoch 3 | Train Loss: 0.3623 | Val Loss: 0.1934 | Train Acc: 0.8969 | Val Acc: 1.0000\n",
      "✅ New best model saved at epoch 3\n",
      "Epoch 4 | Train Loss: 0.3143 | Val Loss: 0.1697 | Train Acc: 0.9103 | Val Acc: 1.0000\n",
      "✅ New best model saved at epoch 4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "upstream = UpstreamExpert(\"small.ckpt\")\n",
    "upstream.to(device)\n",
    "for p in upstream.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "model = AudioClassifier(upstream).to(device)\n",
    "\n",
    "csv_path = r\"C:\\Users\\YIDAN\\Desktop\\projects\\dysarthria\\dataset_youtube_splits_binary.csv\"\n",
    "\n",
    "train_ds = AudioDataset(csv_path, split=\"train\")\n",
    "val_ds = AudioDataset(csv_path, split=\"valid\")\n",
    "train_dl = DataLoader(train_ds, batch_size=2, shuffle=True, collate_fn=collate_fn, pin_memory=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=2, shuffle=False, collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model_path = \"best_model.pth\"\n",
    "training_stats = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_loss, train_acc = train(model, train_dl, optimizer, criterion, device)\n",
    "    val_loss, val_acc = evaluate(model, val_dl, device, criterion=criterion)\n",
    "\n",
    "    print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "          f\"Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    training_stats.append({\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"val_acc\": val_acc,\n",
    "    })\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"✅ New best model saved at epoch {epoch}\")\n",
    "\n",
    "df_stats = pd.DataFrame(training_stats)\n",
    "df_stats.to_csv(\"training_log.csv\", index=False)\n",
    "print(\"✅ Training stats saved to training_log.csv\")\n",
    "\n",
    "# Final evaluation on test set\n",
    "test_ds = AudioDataset(csv_path, split=\"test\")\n",
    "test_dl = DataLoader(test_ds, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate on test set\n",
    "evaluate(model, test_dl, device, output_csv=\"test_predictions.csv\")\n",
    "print(\"✅ Test predictions saved to test_predictions.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314756c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvs3prl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
